{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuuEu_O-Gh0q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data_preparation\n",
        "dx   = pd.read_csv('hosp/diagnoses_icd.csv')\n",
        "adm  = pd.read_csv('hosp/admissions.csv', parse_dates=['admittime'])\n",
        "meta = pd.read_csv('hosp/d_icd_diagnoses.csv')\n",
        "admissions_number = dx.groupby(by='subject_id').apply(lambda x: x['hadm_id'].nunique())\n",
        "icds_number = dx.groupby(by='subject_id').apply(lambda x: x['icd_code'].count())\n",
        "\n",
        "# merge dx → attach each diagnosis to its admission time\n",
        "dx = dx.merge(adm[['hadm_id','admittime', 'dischtime', 'admission_type', 'admission_location', 'discharge_location', 'race', 'hospital_expire_flag']], on='hadm_id')\n",
        "dx = dx.sort_values(['subject_id','admittime'])\n",
        "\n",
        "disease_counts = dx.groupby('subject_id')['icd_code'].nunique()\n",
        "valid_ids = disease_counts[disease_counts > 1].index\n",
        "df = dx[dx['subject_id'].isin(valid_ids)].copy()\n",
        "\n",
        "# Filter only ICD‑10 codes\n",
        "df = df[df['icd_version'] == 10].copy()\n",
        "\n",
        "# Truncate ICD‑10 codes to first 3 characters (e.g., 'C787' → 'C78')\n",
        "df['icd3'] = df['icd_code'].str[:3]\n",
        "df['icd_code'] = df['icd3']\n",
        "df.drop(columns=['icd3'], inplace=True)\n"
      ],
      "metadata": {
        "id": "3xIcZvZzRX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Final_version\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "class Config:\n",
        "    embedding_dim = 256\n",
        "    num_heads     = 8\n",
        "    hidden_dim    = 512\n",
        "    num_layers    = 3\n",
        "    mask_prob     = 0.15\n",
        "    batch_size    = 64\n",
        "    learning_rate = 1e-4\n",
        "    epochs        = 40\n",
        "    max_seq_len   = 100\n",
        "    dropout_rate  = 0.2\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.register_buffer('pe', self._build_pe(max_len), persistent=False)\n",
        "\n",
        "    def _build_pe(self, length):\n",
        "        position = torch.arange(length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model)\n",
        "        )\n",
        "        pe = torch.zeros(length, self.d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  # (1, length, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        if seq_len > self.pe.size(1):\n",
        "            pe_big = self._build_pe(seq_len).to(x.device)\n",
        "            return x + pe_big\n",
        "        else:\n",
        "            return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "class ICDMLMModel(nn.Module):\n",
        "    def __init__(self, config, vocab_size):\n",
        "        super().__init__()\n",
        "        self.cfg = config\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, config.embedding_dim, padding_idx=0)\n",
        "        self.pos_encoder = PositionalEncoding(config.embedding_dim, config.max_seq_len)\n",
        "        self.embedding_dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.embedding_dim,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=config.hidden_dim,\n",
        "            dropout=config.dropout_rate,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=config.num_layers)\n",
        "        self.layer_norm  = nn.LayerNorm(config.embedding_dim)\n",
        "\n",
        "        self.diagnosis_head = nn.Sequential(\n",
        "            nn.Linear(config.embedding_dim, config.hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(config.hidden_dim),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None: nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)          # (B, L, D)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.layer_norm(x)\n",
        "        return self.diagnosis_head(x)          # (B, L, V)\n",
        "\n",
        "def prepare_data(df):\n",
        "    sequences = [\n",
        "        grp.sort_values('seq_num')['disease_id'].tolist()\n",
        "        for _, grp in df.groupby('hadm_id')\n",
        "    ]\n",
        "    vocab = {'[PAD]': 0, '[MASK]': 1}\n",
        "    idx = 2\n",
        "    for seq in sequences:\n",
        "        for code in seq:\n",
        "            if code not in vocab:\n",
        "                vocab[code] = idx\n",
        "                idx += 1\n",
        "    idx_to_code = {i: c for c, i in vocab.items()}\n",
        "    return sequences, vocab, idx_to_code\n",
        "\n",
        "class MaskedICDDataset(Dataset):\n",
        "    def __init__(self, sequences, vocab, cfg):\n",
        "        self.sequences = sequences\n",
        "        self.vocab     = vocab\n",
        "        self.cfg       = cfg\n",
        "        self.mask_id   = vocab['[MASK]']\n",
        "        self.pad_id    = vocab['[PAD]']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx][:self.cfg.max_seq_len]\n",
        "        ids = [self.vocab[c] for c in seq]\n",
        "        pad = [self.pad_id] * (self.cfg.max_seq_len - len(ids))\n",
        "        input_ids = ids + pad\n",
        "\n",
        "        labels = [-100] * self.cfg.max_seq_len\n",
        "        for i in range(len(ids)):\n",
        "            if random.random() < self.cfg.mask_prob:\n",
        "                labels[i] = input_ids[i]\n",
        "                r = random.random()\n",
        "                if r < 0.8:\n",
        "                    input_ids[i] = self.mask_id\n",
        "                elif r < 0.9:\n",
        "                    input_ids[i] = random.choice(list(self.vocab.values()))\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'labels':    torch.tensor(labels,    dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def calculate_accuracy(logits, labels, ignore_index=-100):\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    mask  = labels != ignore_index\n",
        "    correct = (preds[mask] == labels[mask]).sum().float()\n",
        "    total   = mask.sum().float()\n",
        "    return (correct / total).item() if total > 0 else 0.0\n",
        "\n",
        "def get_optimizer(model, cfg, train_loader):\n",
        "    opt = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=0.01)\n",
        "    sched = optim.lr_scheduler.OneCycleLR(\n",
        "        opt,\n",
        "        max_lr=cfg.learning_rate,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=cfg.epochs,\n",
        "        anneal_strategy='linear'\n",
        "    )\n",
        "    return opt, sched\n",
        "\n",
        "def extract_embeddings(model, idx_to_code, vocab):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    valid = [i for i, c in idx_to_code.items() if c not in ('[PAD]', '[MASK]')]\n",
        "    codes = [idx_to_code[i] for i in valid]\n",
        "    input_ids = torch.tensor(valid, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = model.embedding(input_ids)\n",
        "        x = model.pos_encoder(x)\n",
        "        x = model.transformer(x)\n",
        "        x = model.layer_norm(x)\n",
        "        emb = x.squeeze(0).cpu().numpy()\n",
        "\n",
        "    dist = cosine_distances(emb)\n",
        "    return pd.DataFrame(dist, index=codes, columns=codes)\n",
        "\n",
        "def train_mlm(df):\n",
        "    cfg = Config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    sequences, vocab, idx_to_code = prepare_data(df)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Сплит 90/10\n",
        "    n_train = int(0.9 * len(sequences))\n",
        "    train_ds = MaskedICDDataset(sequences[:n_train], vocab, cfg)\n",
        "    val_ds   = MaskedICDDataset(sequences[n_train:],   vocab, cfg)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    model = ICDMLMModel(cfg, vocab_size).to(device)\n",
        "    optimizer, scheduler = get_optimizer(model, cfg, train_loader)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    best_state    = None\n",
        "\n",
        "    for epoch in range(1, cfg.epochs+1):\n",
        "        model.train()\n",
        "        train_loss = train_acc = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Train {epoch}/{cfg.epochs}\"):\n",
        "            inp, lbl = batch['input_ids'].to(device), batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out  = model(inp)\n",
        "            loss = criterion(out.view(-1, vocab_size), lbl.view(-1))\n",
        "            acc  = calculate_accuracy(out, lbl)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item()\n",
        "            train_acc  += acc\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Valid {epoch}/{cfg.epochs}\"):\n",
        "                inp, lbl = batch['input_ids'].to(device), batch['labels'].to(device)\n",
        "                out  = model(inp)\n",
        "                loss = criterion(out.view(-1, vocab_size), lbl.view(-1))\n",
        "                acc  = calculate_accuracy(out, lbl)\n",
        "                val_loss += loss.item()\n",
        "                val_acc  += acc\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_train_acc  = train_acc  / len(train_loader)\n",
        "        avg_val_loss   = val_loss   / len(val_loader)\n",
        "        avg_val_acc    = val_acc    / len(val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch} Summary:\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f}\")\n",
        "        print(f\" Val  Loss: {avg_val_loss:.4f} |  Val  Acc: {avg_val_acc:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_acc  = avg_val_acc\n",
        "            best_state    = model.state_dict().copy()\n",
        "            print(\"New best model saved!\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab':            vocab,\n",
        "        'config':           cfg.__dict__,\n",
        "        'val_loss':         best_val_loss,\n",
        "        'val_acc':          best_val_acc\n",
        "    }, 'icd_mlm_model.pth')\n",
        "    print(\"Модель сохранена в 'icd_mlm_model.pth'\")\n",
        "\n",
        "    return extract_embeddings(model, idx_to_code, vocab)\n"
      ],
      "metadata": {
        "id": "Q-Qm4PuMQU29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    df['disease_id'] = df['icd_code'] + '_v' + df['icd_version'].astype(str)\n",
        "    distance_matrix = train_mlm(df)\n",
        "distance_matrix.to_csv('icd_distance_matrix.csv')\n",
        "print(\"\\nМатрица сходства сохранена в 'icd_distance_matrix.csv'\")"
      ],
      "metadata": {
        "id": "OyxyM7YuQcFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}